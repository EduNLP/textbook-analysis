{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "from gensim.models import CoherenceModel\n",
    "from helpers import *\n",
    "import nltk\n",
    "import json\n",
    "import numpy as np\n",
    "import operator\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 50\n",
    "MALLET = \"/Users/ddemszky/mallet-2.0.8/bin/mallet\"\n",
    "RANDOM_SEED = 42\n",
    "output_dir = \"topics/gensim_\" + str(NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting books...\n",
      "America_A_Narrative_History_WWNorton_10th\n",
      "America_Past_And_Present_Pearson_10th\n",
      "Americas_History_Bedford_8th\n",
      "Give_Me_Liberty_An_American_History_WWNorton_3rd\n",
      "The_American_Pageant_Cengage_14th\n",
      "The_Unfinished_Nation_A_Concise_History_of_the_American_People_McGraw-Hill_8th\n",
      "Visions_of_America_A_History_of_the_United_States_Pearson_2nd\n",
      "american_history_connecting_with_the_past\n",
      "by_the_people\n",
      "history_alive_united_states_thru_industrialism\n",
      "hmh_the_americans_us_history_since_1877\n",
      "mastering_the_teks\n",
      "pearson_us_history\n",
      "teks_us_history\n",
      "us_history_early_colonial_period_through_reconstruction\n"
     ]
    }
   ],
   "source": [
    "books = get_book_txts(\"data/coref_resolved_txts\", splitlines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and combining texts...\n",
      "America_A_Narrative_History_WWNorton_10th\n",
      "America_Past_And_Present_Pearson_10th\n",
      "Americas_History_Bedford_8th\n",
      "Give_Me_Liberty_An_American_History_WWNorton_3rd\n",
      "The_American_Pageant_Cengage_14th\n",
      "The_Unfinished_Nation_A_Concise_History_of_the_American_People_McGraw-Hill_8th\n",
      "Visions_of_America_A_History_of_the_United_States_Pearson_2nd\n",
      "american_history_connecting_with_the_past\n",
      "by_the_people\n",
      "history_alive_united_states_thru_industrialism\n",
      "hmh_the_americans_us_history_since_1877\n",
      "mastering_the_teks\n",
      "pearson_us_history\n",
      "teks_us_history\n",
      "us_history_early_colonial_period_through_reconstruction\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning and combining texts...\")\n",
    "all_sentences = []\n",
    "start_end = []\n",
    "prev = 0\n",
    "for title, book in books.items():\n",
    "    print(title)\n",
    "    sents = nltk.sent_tokenize(book)\n",
    "    start = prev\n",
    "    for i, s in enumerate(sents):\n",
    "        if len(s) < 15:\n",
    "            continue\n",
    "        all_sentences.append(clean_text(s, stem=True, remove_short=True))\n",
    "    end = start + len(sents) - 1\n",
    "    start_end.append((title, start, end))\n",
    "    prev = end + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_end_dict = {}\n",
    "for tup in start_end:\n",
    "    start_end_dict[tup[0]] = (tup[1], tup[2])\n",
    "with open(output_dir + '/book_start_end.json', 'w') as f:\n",
    "    f.write(json.dumps(start_end_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316124 sentences total\n"
     ]
    }
   ],
   "source": [
    "print(\"%d sentences total\" % len(all_sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dictionary...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating dictionary...\")\n",
    "id2word = corpora.Dictionary(all_sentences)\n",
    "id2word.save(output_dir + '/dictionary.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting term-document frequencies...\n"
     ]
    }
   ],
   "source": [
    "print(\"Getting term-document frequencies...\")\n",
    "corpus = [id2word.doc2bow(t) for t in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topics(num, corpus, id2word, output_dir, all_sentences):\n",
    "    print(num)\n",
    "    ldamallet = LdaMallet(MALLET,\n",
    "                          corpus=corpus,\n",
    "                          num_topics=num,\n",
    "                          prefix=output_dir + \"/\",\n",
    "                          workers=1,   # workers has to be 1, otherwise you get a java out of bounds exception\n",
    "                          id2word=id2word,\n",
    "                          iterations=1000,\n",
    "                          random_seed=RANDOM_SEED,\n",
    "                         alpha=5)\n",
    "    coherence_model_ldamallet = CoherenceModel(model=ldamallet,\n",
    "                                               texts=all_sentences,\n",
    "                                               dictionary=id2word,\n",
    "                                               coherence='c_v')\n",
    "    coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "    print('\\nCoherence Score: ', coherence_ldamallet)\n",
    "    keywords = {i: \", \".join([word for word, prop in ldamallet.show_topic(i)]) for i in range(ldamallet.num_topics)}\n",
    "    with open(output_dir + \"/topic_names.json\", 'w') as f:\n",
    "        f.write(json.dumps(keywords))\n",
    "    ldamallet.save(output_dir + \"/model.mallet\")\n",
    "    ldamallet.show_topics(num_topics=num, formatted=True)\n",
    "    return coherence_ldamallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running topic model with 50 topics...\n",
      "50\n",
      "\n",
      "Coherence Score:  0.6697524666698742\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6697524666698742"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Running topic model with %d topics...\" % NUM_TOPICS)\n",
    "get_topics(NUM_TOPICS, corpus, id2word, output_dir, all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamallet = LdaMallet.load(output_dir + \"/model.mallet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet,\n",
    "                                               texts=all_sentences,\n",
    "                                               dictionary=id2word,\n",
    "                                               coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ldamallet.show_topics(num_topics=NUM_TOPICS, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic_file =  output_dir+ '/doctopics.txt'\n",
    "doc_topics = open(doc_topic_file).read().splitlines()\n",
    "print(len(doc_topics), 'articles total')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names = json.load(open(output_dir + '/topic_names.json', 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prominence as measured by topic keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_keys = open(output_dir+ '/topickeys.txt').read().splitlines()\n",
    "topic2weight = {}\n",
    "for t in topic_keys:\n",
    "    topic2weight[topic_names[t.split()[0]]] = float(t.split()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_topics = sorted(topic2weight.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prominence as measured by averaging the topic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic_mat = np.array([[float(n) for n in l.strip().split(\"\\t\")[2:]] for l in doc_topics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_topic_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in doc_topic_mat.mean(axis=0).argsort()[::-1]:\n",
    "    print(doc_topic_mat.mean(axis=0)[t], topic_names[str(t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prominence based on averaging across books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = [\"America_A_Narrative_History_WWNorton_10th\",\n",
    "          \"America_Past_And_Present_Pearson_10th\",\n",
    "          \"american_history_connecting_with_the_past\",\n",
    "          \"Americas_History_Bedford_8th\",\n",
    "          \"by_the_people\",\"Give_Me_Liberty_An_American_History_WWNorton_3rd\",\n",
    "          \"history_alive_united_states_thru_industrialism\",\n",
    "          \"hmh_the_americans_us_history_since_1877\",\"mastering_the_teks\",\"pearson_us_history\",\"teks_us_history\",\"The_American_Pageant_Cengage_14th\",\"The_Unfinished_Nation_A_Concise_History_of_the_American_People_McGraw-Hill_8th\",\"us_history_early_colonial_period_through_reconstruction\",\"Visions_of_America_A_History_of_the_United_States_Pearson_2nd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#titles = [t for t in titles if not ('early' in t or 'industr' in t or 'since' in t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_means = []\n",
    "for title in titles:\n",
    "    start, end = start_end_dict[title]\n",
    "    doc_topics_book = doc_topics[start:end]\n",
    "    book_means.append(np.array([[float(n) for n in l.strip().split(\"\\t\")[2:]] for l in doc_topics_book]).mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in np.array(book_means).mean(axis=0).argsort()[::-1]:\n",
    "    print(np.array(book_means).mean(axis=0)[t], topic_names[str(t)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topic_for_doc(doc_id, printout=True):\n",
    "    doc = all_sentences[doc_id]\n",
    "    if printout:\n",
    "        print(doc)\n",
    "        #print(doc_topics[doc_id])\n",
    "    topics = doc_topics[doc_id].strip().split()[2:]\n",
    "    topics = set([i for (i, v) in enumerate(topics)\n",
    "                         if float(v) > 0.1])\n",
    "    if printout:\n",
    "        print(\"Topics:\")\n",
    "        for t in topics:\n",
    "            print(topic_names[str(t)])\n",
    "    return len(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_topic_for_doc(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topic_for_word(word):\n",
    "    topics = []\n",
    "    for k, v in topic_names.items():\n",
    "        if word in v:\n",
    "            print(k, v)\n",
    "            topics.append(int(k))\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_topic_for_word(\"wom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_topic_for_word(\"men,\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By most prominent topic(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_counts = np.zeros(NUM_TOPICS)\n",
    "for i in range(NUM_TOPICS):\n",
    "    topic_counts[i] = (doc_topic_mat[:, i] > 0.1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for t in topic_counts.argsort()[::-1]:\n",
    "    print(topic_counts[t], topic_names[str(t)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape_mapper = get_shapes(abbr=True)\n",
    "color_mapper = get_colors(abbr=True)\n",
    "title_abbr = get_title_abbr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ratio_of_topic_prominence(topic1, topic2, name1, name2):\n",
    "    df_ratio = []\n",
    "    df_book = []\n",
    "    df_dem = []\n",
    "    for i, t in enumerate(titles):\n",
    "        # remove books that only cover half of US history\n",
    "        if 'early' in t or 'industr' in t or 'since' in t:\n",
    "            continue\n",
    "        df_book.append(title_abbr[t])\n",
    "        df_dem.append(dem_per_book[t])\n",
    "        df_ratio.append(book_means[i][topic1] / book_means[i][topic2])\n",
    "    return pd.DataFrame({'Book': df_book, 'ratio': df_ratio, 'dem': df_dem})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_topic_for_word(\"slaveri\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_topic_for_word(\"milit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/dem_per_book.json', 'r') as f:\n",
    "    dem_per_book = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name1 = \"Slavery\"\n",
    "name2 = \"Military\"\n",
    "slavery_df = get_ratio_of_topic_prominence(29, 45, name1, name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ratio_of_topic_group_prominence(topics_1, topics_2):\n",
    "    df_ratio = []\n",
    "    df_book = []\n",
    "    df_dem = []\n",
    "    df_minratio = []\n",
    "    df_maxratio = []\n",
    "    for i, t in enumerate(titles):\n",
    "        if 'early' in t or 'industr' in t or 'since' in t:\n",
    "            continue\n",
    "        df_book.append(title_abbr[t])\n",
    "        df_dem.append(dem_per_book[t])\n",
    "        df_ratio.append(np.sum([book_means[i][topic1] for topic1 in topics_1]) / \n",
    "                        np.sum([book_means[i][topic2] for topic2 in topics_2]))\n",
    "        \n",
    "        # get leave-out values\n",
    "        ratios = []\n",
    "        for t1 in topics_1:\n",
    "            for t2 in topics_2:\n",
    "                ratios.append(book_means[i][t1] / book_means[i][t2])\n",
    "        df_minratio.append(min(ratios))\n",
    "        df_maxratio.append(max(ratios))\n",
    "        \n",
    "    return pd.DataFrame({'Book': df_book, 'ratio': df_ratio, 'dem': df_dem, 'min_ratio': df_minratio, 'max_ratio': df_maxratio})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "women_topics = get_topic_for_word(\"women\")\n",
    "president_topics = get_topic_for_word(\"presid\")\n",
    "women_df = get_ratio_of_topic_group_prominence(women_topics, president_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "women_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
